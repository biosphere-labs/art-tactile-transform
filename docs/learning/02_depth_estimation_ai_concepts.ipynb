{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth Estimation & AI Concepts\n",
    "\n",
    "Welcome to the second notebook in our Art Tactile Transform series! This notebook dives deep into the AI and computer vision concepts that make depth estimation possible.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand how AI models estimate depth from 2D images\n",
    "- Learn about different depth estimation architectures\n",
    "- Explore Hugging Face's model ecosystem\n",
    "- Implement custom depth estimation workflows\n",
    "- Compare different models and their trade-offs\n",
    "\n",
    "## ðŸ§  What is Depth Estimation?\n",
    "\n",
    "Depth estimation is the process of predicting the distance of objects from the camera for each pixel in an image. It's a fundamental computer vision task that bridges 2D images and 3D understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "print(\"ðŸ§  Depth Estimation Fundamentals\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "depth_concepts = {\n",
    "    \"Monocular Depth\": \"Depth estimation from a single image (what we use)\",\n",
    "    \"Stereo Depth\": \"Using two cameras to calculate depth (like human vision)\",\n",
    "    \"Structured Light\": \"Projecting patterns to measure depth (like Kinect)\",\n",
    "    \"Time-of-Flight\": \"Measuring light travel time (like LiDAR)\",\n",
    "    \"Relative Depth\": \"Ordering objects by distance (closer/farther)\",\n",
    "    \"Metric Depth\": \"Absolute distance measurements in real units\"\n",
    "}\n",
    "\n",
    "for concept, explanation in depth_concepts.items():\n",
    "    print(f\"ðŸ“Š {concept:<15} - {explanation}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Why Monocular Depth Estimation?\")\n",
    "print(\"â€¢ Works with any single image (no special hardware)\")\n",
    "print(\"â€¢ AI models trained on millions of image-depth pairs\")\n",
    "print(\"â€¢ Leverages visual cues humans use: shadows, perspective, occlusion\")\n",
    "print(\"â€¢ Perfect for our tactile transformation use case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ AI Architecture Deep Dive\n",
    "\n",
    "Let's explore the neural network architectures commonly used for depth estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ—ï¸ Depth Estimation AI Architectures\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "architectures = {\n",
    "    \"CNN Encoder-Decoder\": {\n",
    "        \"description\": \"Classic architecture with downsampling and upsampling\",\n",
    "        \"pros\": [\"Simple\", \"Fast inference\", \"Good for real-time\"],\n",
    "        \"cons\": [\"Limited long-range dependencies\", \"May lose fine details\"],\n",
    "        \"examples\": [\"DenseDepth\", \"FastDepth\"]\n",
    "    },\n",
    "    \"Vision Transformers (ViT)\": {\n",
    "        \"description\": \"Attention-based models treating images as sequences\",\n",
    "        \"pros\": [\"Global context\", \"High accuracy\", \"Transfer learning\"],\n",
    "        \"cons\": [\"Computationally expensive\", \"Needs large datasets\"],\n",
    "        \"examples\": [\"DPT (Dense Prediction Transformer)\", \"BEiT\"]\n",
    "    },\n",
    "    \"Hybrid CNN-Transformer\": {\n",
    "        \"description\": \"Combines CNN feature extraction with transformer reasoning\",\n",
    "        \"pros\": [\"Best of both worlds\", \"Efficient\", \"High quality\"],\n",
    "        \"cons\": [\"Complex architecture\", \"More parameters\"],\n",
    "        \"examples\": [\"Intel DPT models\", \"Swin Transformer\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for arch_name, info in architectures.items():\n",
    "    print(f\"\\nðŸ”§ {arch_name}\")\n",
    "    print(f\"   ðŸ“ {info['description']}\")\n",
    "    print(f\"   âœ… Pros: {', '.join(info['pros'])}\")\n",
    "    print(f\"   âš ï¸  Cons: {', '.join(info['cons'])}\")\n",
    "    print(f\"   ðŸ·ï¸  Examples: {', '.join(info['examples'])}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Our Choice: Intel DPT Models\")\n",
    "print(\"We use Intel's DPT (Dense Prediction Transformer) models because:\")\n",
    "print(\"â€¢ State-of-the-art accuracy\")\n",
    "print(\"â€¢ Pre-trained on diverse datasets\")\n",
    "print(\"â€¢ Available through Hugging Face\")\n",
    "print(\"â€¢ Good balance of quality and speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤— Hugging Face Model Ecosystem\n",
    "\n",
    "Let's explore the available depth estimation models on Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ¤— Hugging Face Depth Models\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "hf_models = {\n",
    "    \"Intel/dpt-large\": {\n",
    "        \"description\": \"Large DPT model, highest quality\",\n",
    "        \"size\": \"~1.3GB\",\n",
    "        \"speed\": \"Slow\",\n",
    "        \"quality\": \"Excellent\",\n",
    "        \"use_case\": \"Best results, offline processing\"\n",
    "    },\n",
    "    \"Intel/dpt-hybrid-midas\": {\n",
    "        \"description\": \"Hybrid model balancing speed and quality\",\n",
    "        \"size\": \"~400MB\",\n",
    "        \"speed\": \"Medium\",\n",
    "        \"quality\": \"Very Good\",\n",
    "        \"use_case\": \"Good balance for most applications\"\n",
    "    },\n",
    "    \"Intel/dpt-swinv2-base-384\": {\n",
    "        \"description\": \"Swin Transformer v2 based model\",\n",
    "        \"size\": \"~350MB\",\n",
    "        \"speed\": \"Medium\",\n",
    "        \"quality\": \"Very Good\",\n",
    "        \"use_case\": \"Modern architecture, good performance\"\n",
    "    },\n",
    "    \"facebook/dpt-dinov2-base-kitti\": {\n",
    "        \"description\": \"DINOv2 based, trained on KITTI dataset\",\n",
    "        \"size\": \"~300MB\",\n",
    "        \"speed\": \"Fast\",\n",
    "        \"quality\": \"Good\",\n",
    "        \"use_case\": \"Automotive/outdoor scenes\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name, info in hf_models.items():\n",
    "    print(f\"\\nðŸ¤– {model_name}\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"   {key.capitalize():<12}: {value}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Model Selection Tips:\")\n",
    "print(\"â€¢ Start with Intel/dpt-large for best quality\")\n",
    "print(\"â€¢ Use Intel/dpt-hybrid-midas for faster processing\")\n",
    "print(\"â€¢ Consider facebook/dpt-dinov2-base-kitti for outdoor scenes\")\n",
    "print(\"â€¢ Test different models to see which works best for your images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Understanding the API Communication\n",
    "\n",
    "Let's examine how our application communicates with Hugging Face's Inference API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine our API communication function\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project to Python path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "from art_tactile_transform.main import query_hf_api\n",
    "import inspect\n",
    "\n",
    "print(\"ðŸ”¬ API Communication Analysis\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Show the function signature and code\n",
    "print(\"ðŸ“‹ Function Signature:\")\n",
    "print(inspect.signature(query_hf_api))\n",
    "\n",
    "print(\"\\nðŸ“ Function Source Code:\")\n",
    "print(inspect.getsource(query_hf_api))\n",
    "\n",
    "print(\"\\nðŸ”§ Key Components Explained:\")\n",
    "components = {\n",
    "    \"API_URL\": \"Base URL for HuggingFace Inference API\",\n",
    "    \"Headers\": \"Authentication using Bearer token\",\n",
    "    \"POST Request\": \"Sends raw image bytes to the model\",\n",
    "    \"Timeout\": \"30 second limit to prevent hanging\",\n",
    "    \"Error Handling\": \"Catches and re-raises network errors\",\n",
    "    \"Response\": \"Returns raw depth map image bytes\"\n",
    "}\n",
    "\n",
    "for component, explanation in components.items():\n",
    "    print(f\"â€¢ {component:<15}: {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experiment: Compare Different Models\n",
    "\n",
    "Let's create a function to test different depth estimation models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_scene():\n",
    "    \"\"\"Create a test image with clear depth cues.\"\"\"\n",
    "    img = Image.new('RGB', (256, 256), 'lightblue')  # Sky background\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Draw a simple scene with depth\n",
    "    # Far mountains (dark)\n",
    "    points = [(0, 180), (50, 160), (100, 170), (150, 150), (200, 160), (256, 170), (256, 256), (0, 256)]\n",
    "    draw.polygon(points, fill='darkslategray')\n",
    "    \n",
    "    # Middle ground trees (medium)\n",
    "    for x in [60, 120, 200]:\n",
    "        draw.ellipse([x-15, 140, x+15, 200], fill='darkgreen')\n",
    "        draw.rectangle([x-3, 190, x+3, 220], fill='brown')\n",
    "    \n",
    "    # Foreground objects (light)\n",
    "    draw.ellipse([20, 200, 80, 240], fill='lightgreen')  # Bush\n",
    "    draw.rectangle([180, 180, 220, 240], fill='gray')   # Rock\n",
    "    \n",
    "    # Road leading into distance\n",
    "    road_points = [(100, 256), (140, 256), (130, 180), (120, 180)]\n",
    "    draw.polygon(road_points, fill='darkgray')\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create our test scene\n",
    "test_scene = create_test_scene()\n",
    "test_path = project_root / \"test_scene.png\"\n",
    "test_scene.save(test_path)\n",
    "\n",
    "print(\"ðŸ§ª Depth Model Comparison Experiment\")\n",
    "print(\"=\"*45)\n",
    "print(f\"âœ… Created test scene: {test_path}\")\n",
    "print(\"ðŸ“Š Scene includes:\")\n",
    "print(\"  â€¢ Far mountains (should be dark in depth map)\")\n",
    "print(\"  â€¢ Middle trees (medium depth)\")\n",
    "print(\"  â€¢ Foreground objects (should be bright in depth map)\")\n",
    "print(\"  â€¢ Road with perspective (depth gradient)\")\n",
    "\n",
    "# Display the test scene\n",
    "try:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(test_scene)\n",
    "    plt.title(\"Test Scene for Depth Estimation\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "except:\n",
    "    print(\"ðŸ“Š Test scene created (install matplotlib to visualize)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Exercise: Test Different Models\")\n",
    "print(\"1. Update MODEL_NAME in .env to different models\")\n",
    "print(\"2. Set IMAGE_PATH to point to test_scene.png\")\n",
    "print(\"3. Run the transformation with each model\")\n",
    "print(\"4. Compare the resulting depth maps and STL files\")\n",
    "print(\"5. Note differences in processing time and quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Understanding Depth Map Visualization\n",
    "\n",
    "Depth maps are grayscale images where pixel intensity represents distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_depth_map(width=128, height=128):\n",
    "    \"\"\"Create a simulated depth map to understand the concept.\"\"\"\n",
    "    # Create coordinate grids\n",
    "    x = np.linspace(-1, 1, width)\n",
    "    y = np.linspace(-1, 1, height)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Create different depth patterns\n",
    "    patterns = {\n",
    "        \"Sphere\": np.sqrt(X**2 + Y**2),\n",
    "        \"Plane\": Y,\n",
    "        \"Cone\": np.sqrt(X**2 + Y**2),\n",
    "        \"Waves\": np.sin(X * 5) * np.cos(Y * 5),\n",
    "        \"Pyramid\": np.maximum(np.abs(X), np.abs(Y))\n",
    "    }\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "print(\"ðŸŽ¨ Depth Map Visualization Concepts\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "depth_patterns = simulate_depth_map()\n",
    "\n",
    "print(\"ðŸ“Š Depth Map Conventions:\")\n",
    "print(\"â€¢ White/Light = Close to camera (high depth values)\")\n",
    "print(\"â€¢ Black/Dark = Far from camera (low depth values)\")\n",
    "print(\"â€¢ Gray = Medium distance\")\n",
    "print(\"â€¢ Smooth gradients = Gentle slopes\")\n",
    "print(\"â€¢ Sharp edges = Sudden depth changes\")\n",
    "\n",
    "try:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (name, pattern) in enumerate(depth_patterns.items()):\n",
    "        if i < len(axes):\n",
    "            # Normalize to 0-1 range\n",
    "            normalized = (pattern - pattern.min()) / (pattern.max() - pattern.min())\n",
    "            \n",
    "            axes[i].imshow(normalized, cmap='gray')\n",
    "            axes[i].set_title(f\"{name} Depth Pattern\")\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide the last subplot if odd number\n",
    "    if len(depth_patterns) < len(axes):\n",
    "        axes[-1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Different Depth Pattern Examples\", y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ðŸ“Š Patterns visualized above\")\n",
    "except:\n",
    "    print(\"ðŸ“Š Install matplotlib to see depth pattern visualizations\")\n",
    "\n",
    "print(\"\\nðŸ” What to Look For in Real Depth Maps:\")\n",
    "print(\"â€¢ Objects in foreground should be brighter\")\n",
    "print(\"â€¢ Background should be darker\")\n",
    "print(\"â€¢ Depth should change smoothly across surfaces\")\n",
    "print(\"â€¢ Sharp depth edges at object boundaries\")\n",
    "print(\"â€¢ Consistent lighting-independent depth estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Custom Model Testing Framework\n",
    "\n",
    "Let's build a framework to systematically test different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class DepthModelTester:\n",
    "    \"\"\"A class to systematically test different depth estimation models.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        load_dotenv(project_root / \".env\")\n",
    "        self.api_token = os.getenv(\"HF_API_TOKEN\")\n",
    "        self.test_results = []\n",
    "    \n",
    "    def test_model(self, model_name, image_path, timeout=60):\n",
    "        \"\"\"Test a single model and record performance metrics.\"\"\"\n",
    "        print(f\"\\nðŸ§ª Testing model: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load test image\n",
    "            with open(image_path, 'rb') as f:\n",
    "                image_bytes = f.read()\n",
    "            \n",
    "            # Time the API call\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # This is a mock - in real usage you'd call query_hf_api\n",
    "            # depth_bytes = query_hf_api(image_bytes, model_name, self.api_token)\n",
    "            \n",
    "            # Simulate API call for demonstration\n",
    "            time.sleep(2)  # Simulate processing time\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'processing_time': processing_time,\n",
    "                'status': 'success',\n",
    "                'image_size': len(image_bytes)\n",
    "            }\n",
    "            \n",
    "            print(f\"   âœ… Success - {processing_time:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'model': model_name,\n",
    "                'processing_time': None,\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "            print(f\"   âŒ Failed - {e}\")\n",
    "        \n",
    "        self.test_results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def run_model_comparison(self, models, image_path):\n",
    "        \"\"\"Test multiple models and compare results.\"\"\"\n",
    "        print(\"ðŸ”¬ Model Comparison Test Suite\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        for model in models:\n",
    "            self.test_model(model, image_path)\n",
    "        \n",
    "        self.print_comparison_report()\n",
    "    \n",
    "    def print_comparison_report(self):\n",
    "        \"\"\"Print a comparison report of all tested models.\"\"\"\n",
    "        print(\"\\nðŸ“Š Model Comparison Report\")\n",
    "        print(\"=\"*35)\n",
    "        \n",
    "        successful_results = [r for r in self.test_results if r['status'] == 'success']\n",
    "        \n",
    "        if successful_results:\n",
    "            # Sort by processing time\n",
    "            successful_results.sort(key=lambda x: x['processing_time'])\n",
    "            \n",
    "            print(f\"{'Model':<35} {'Time (s)':<10} {'Status':<10}\")\n",
    "            print(\"-\" * 55)\n",
    "            \n",
    "            for result in self.test_results:\n",
    "                model_short = result['model'].split('/')[-1][:30]\n",
    "                time_str = f\"{result['processing_time']:.2f}\" if result['processing_time'] else \"N/A\"\n",
    "                print(f\"{model_short:<35} {time_str:<10} {result['status']:<10}\")\n",
    "            \n",
    "            if successful_results:\n",
    "                fastest = successful_results[0]\n",
    "                slowest = successful_results[-1]\n",
    "                \n",
    "                print(f\"\\nðŸ† Fastest: {fastest['model']} ({fastest['processing_time']:.2f}s)\")\n",
    "                print(f\"ðŸŒ Slowest: {slowest['model']} ({slowest['processing_time']:.2f}s)\")\n",
    "        \n",
    "        failed_results = [r for r in self.test_results if r['status'] == 'failed']\n",
    "        if failed_results:\n",
    "            print(f\"\\nâŒ Failed Models: {len(failed_results)}\")\n",
    "            for result in failed_results:\n",
    "                print(f\"   â€¢ {result['model']}: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Create the tester\n",
    "tester = DepthModelTester()\n",
    "\n",
    "print(\"ðŸ› ï¸ Model Testing Framework\")\n",
    "print(\"=\"*30)\n",
    "print(\"Framework created for systematic model testing.\")\n",
    "print(\"\\nðŸŽ¯ Usage Example:\")\n",
    "print(\"```python\")\n",
    "models_to_test = [\")\n",
    "print('    \"Intel/dpt-large\",\")\n",
    "print('    \"Intel/dpt-hybrid-midas\",\")\n",
    "print('    \"facebook/dpt-dinov2-base-kitti\"')\n",
    "print(\"]\")\n",
    "print('tester.run_model_comparison(models_to_test, \"test_scene.png\")')\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Testing Tips:\")\n",
    "print(\"â€¢ Test with different types of images (indoor, outdoor, artistic)\")\n",
    "print(\"â€¢ Consider network latency in timing measurements\")\n",
    "print(\"â€¢ Some models may be temporarily unavailable\")\n",
    "print(\"â€¢ Quality assessment requires visual inspection of results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Hands-On Exercise: Model Analysis\n",
    "\n",
    "Now it's your turn to analyze and understand depth estimation models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Exercise: Depth Model Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "exercise_tasks = [\n",
    "    {\n",
    "        \"task\": \"Create Test Images\",\n",
    "        \"description\": \"Create 3 different test images with varying complexity\",\n",
    "        \"steps\": [\n",
    "            \"Simple geometric shapes (circles, rectangles)\",\n",
    "            \"Natural scene with foreground/background\",\n",
    "            \"Complex scene with multiple depth layers\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Model Comparison\",\n",
    "        \"description\": \"Test at least 3 different models on each image\",\n",
    "        \"steps\": [\n",
    "            \"Record processing times\",\n",
    "            \"Save depth map outputs\",\n",
    "            \"Generate STL files\",\n",
    "            \"Note any errors or failures\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Quality Assessment\",\n",
    "        \"description\": \"Evaluate the quality of depth estimations\",\n",
    "        \"steps\": [\n",
    "            \"Check if foreground objects are brighter\",\n",
    "            \"Verify smooth depth transitions\",\n",
    "            \"Look for artifacts or noise\",\n",
    "            \"Assess overall realism\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"Performance Analysis\",\n",
    "        \"description\": \"Analyze speed vs quality trade-offs\",\n",
    "        \"steps\": [\n",
    "            \"Plot processing time vs model size\",\n",
    "            \"Identify fastest model for each quality level\",\n",
    "            \"Consider use case requirements\",\n",
    "            \"Make recommendations\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, exercise in enumerate(exercise_tasks, 1):\n",
    "    print(f\"\\nðŸ“‹ Exercise {i}: {exercise['task']}\")\n",
    "    print(f\"   ðŸŽ¯ Goal: {exercise['description']}\")\n",
    "    print(\"   ðŸ“ Steps:\")\n",
    "    for step in exercise['steps']:\n",
    "        print(f\"      â€¢ {step}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Create Your Analysis Report:\")\n",
    "print(\"After completing the exercises, document:\")\n",
    "print(\"â€¢ Which model provides the best quality for your use case?\")\n",
    "print(\"â€¢ What's the speed-quality trade-off sweet spot?\")\n",
    "print(\"â€¢ Are there specific types of images that work better with certain models?\")\n",
    "print(\"â€¢ What would you recommend for different scenarios?\")\n",
    "\n",
    "print(\"\\nðŸ”§ Code Template for Testing:\")\n",
    "test_code = '''\n",
    "# Update .env with model name\n",
    "os.environ['MODEL_NAME'] = 'Intel/dpt-large'\n",
    "os.environ['IMAGE_PATH'] = 'your_test_image.png'\n",
    "os.environ['OUTPUT_PATH'] = 'output_large.stl'\n",
    "\n",
    "# Run transformation\n",
    "from art_tactile_transform.main import generate_3d\n",
    "output_file = generate_3d()\n",
    "print(f\"Generated: {output_file}\")\n",
    "'''\n",
    "print(test_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Advanced Topics: Model Fine-Tuning Concepts\n",
    "\n",
    "For advanced users interested in customizing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¬ Advanced: Model Customization Concepts\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "advanced_topics = {\n",
    "    \"Transfer Learning\": {\n",
    "        \"concept\": \"Adapting pre-trained models to specific domains\",\n",
    "        \"when_useful\": \"Specific image types (medical, artistic, aerial)\",\n",
    "        \"complexity\": \"Medium\",\n",
    "        \"resources_needed\": \"Labeled depth data, GPU for training\"\n",
    "    },\n",
    "    \"Domain Adaptation\": {\n",
    "        \"concept\": \"Adjusting models for different visual domains\",\n",
    "        \"when_useful\": \"Indoor vs outdoor, day vs night\",\n",
    "        \"complexity\": \"High\",\n",
    "        \"resources_needed\": \"Diverse datasets, significant compute\"\n",
    "    },\n",
    "    \"Model Optimization\": {\n",
    "        \"concept\": \"Making models faster/smaller for specific hardware\",\n",
    "        \"when_useful\": \"Real-time applications, edge devices\",\n",
    "        \"complexity\": \"Medium-High\",\n",
    "        \"resources_needed\": \"Optimization frameworks, target hardware\"\n",
    "    },\n",
    "    \"Custom Training\": {\n",
    "        \"concept\": \"Training models from scratch on specific data\",\n",
    "        \"when_useful\": \"Unique domains, specific requirements\",\n",
    "        \"complexity\": \"Very High\",\n",
    "        \"resources_needed\": \"Large datasets, significant compute, expertise\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for topic, info in advanced_topics.items():\n",
    "    print(f\"\\nðŸ§  {topic}\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"   {key.replace('_', ' ').title():<18}: {value}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ For Most Users:\")\n",
    "print(\"â€¢ Pre-trained models work well for general use cases\")\n",
    "print(\"â€¢ Focus on choosing the right existing model\")\n",
    "print(\"â€¢ Experiment with different models before custom training\")\n",
    "print(\"â€¢ Custom training is usually overkill for tactile transformation\")\n",
    "\n",
    "print(\"\\nðŸ”— Resources for Advanced Users:\")\n",
    "print(\"â€¢ Hugging Face Transformers documentation\")\n",
    "print(\"â€¢ Papers: 'Vision Transformers for Dense Prediction'\")\n",
    "print(\"â€¢ Datasets: NYU Depth V2, KITTI, Cityscapes\")\n",
    "print(\"â€¢ Tools: PyTorch, TensorFlow, Hugging Face Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Key Concepts Summary\n",
    "\n",
    "After completing this notebook, you should understand:\n",
    "\n",
    "### âœ… Depth Estimation Fundamentals\n",
    "- **Monocular vs Stereo**: Single image vs two-camera depth estimation\n",
    "- **AI Architectures**: CNN, Transformer, and Hybrid approaches\n",
    "- **Model Trade-offs**: Speed vs Quality vs Size considerations\n",
    "- **Depth Maps**: Grayscale representation of 3D depth information\n",
    "\n",
    "### âœ… Hugging Face Ecosystem\n",
    "- **Model Selection**: Choosing the right model for your use case\n",
    "- **API Integration**: How to communicate with inference endpoints\n",
    "- **Performance Testing**: Systematic model comparison techniques\n",
    "- **Error Handling**: Robust API communication practices\n",
    "\n",
    "### âœ… Practical Skills\n",
    "- **Model Comparison**: Testing multiple models systematically\n",
    "- **Quality Assessment**: Evaluating depth estimation results\n",
    "- **Performance Analysis**: Understanding speed-quality trade-offs\n",
    "- **Test Scene Creation**: Designing images for depth estimation testing\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "Continue your learning journey with:\n",
    "\n",
    "- **03_image_processing_techniques.ipynb** - Learn about enhancing images for better depth estimation\n",
    "- **04_3d_modeling_stl_generation.ipynb** - Understand how depth maps become 3D models\n",
    "- **05_hands_on_exercises.ipynb** - Practice with real coding challenges\n",
    "\n",
    "## ðŸ”¬ Challenge Questions\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "1. **Why might a Vision Transformer perform better than a CNN for depth estimation?**\n",
    "2. **What visual cues do AI models use to estimate depth from a single image?**\n",
    "3. **How would you choose between Intel/dpt-large and Intel/dpt-hybrid-midas for a real-time application?**\n",
    "4. **What preprocessing steps might improve depth estimation quality?**\n",
    "5. **How does the choice of depth model affect the final tactile representation?**\n",
    "\n",
    "---\n",
    "*Ready to dive deeper into image processing? Let's continue! ðŸš€*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.13.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}